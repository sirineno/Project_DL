{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Saliency model based on Resnet-50 and linear probing\n",
    "In this notebook we're gonna go through our model and explain it step by step. \n",
    "\n",
    "We actually ran the train and validation in two different python scripts (see folder Train_and_Val). We wrote This notebook to go through the code explain the steps and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary librairies: \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we loaded the Resnet 50 as a feature extractor and removed its final classification layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load ResNet50 as feature extractor\n",
    "backbone = models.resnet50(pretrained=True)\n",
    "feature_dim = backbone.fc.in_features\n",
    "backbone = nn.Sequential(*list(backbone.children())[:-1])  # Remove final classification layer\n",
    "backbone.to(device).eval()  # Move model to GPU/CPU and set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we defined our Linear probing class that maps the features extracted from ResNet to saliency maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, feature_dim, output_size=(256, 256)):\n",
    "        \"\"\"\n",
    "        Initializes the LinearProbe module.\n",
    "\n",
    "        Args:\n",
    "            feature_dim (int): The dimensionality of the input feature vector.\n",
    "            output_size (tuple): The desired output size of the saliency map (height, width).\n",
    "        \"\"\"\n",
    "        super(LinearProbe, self).__init__()\n",
    "        # Fully connected layer to map features to the desired output size\n",
    "        self.fc = nn.Linear(feature_dim, output_size[0] * output_size[1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LinearProbe.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input feature tensor of shape (batch_size, feature_dim, ...).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A saliency map of shape (batch_size, 1, 256, 256).\n",
    "        \"\"\"\n",
    "        # Flatten the input feature map to (batch_size, feature_dim)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Apply the fully connected layer to map features to a vector of size (256 * 256)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Reshape the output to a saliency map of size (batch_size, 1, 256, 256)\n",
    "        return x.view(x.size(0), 1, 256, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we combined the feature extractor and linear probing class into one class that we called SaliencyPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyPredictor(nn.Module):\n",
    "    def __init__(self, backbone, feature_dim):\n",
    "        \"\"\"\n",
    "        Initializes the SaliencyPredictor model.\n",
    "\n",
    "        Args:\n",
    "            backbone (nn.Module): A pre-trained backbone network (ResNet) \n",
    "                                to extract features from input images.\n",
    "            feature_dim (int): The dimensionality of the feature vector output by the backbone.\n",
    "        \"\"\"\n",
    "        super(SaliencyPredictor, self).__init__()\n",
    "        self.backbone = backbone  # Backbone network for feature extraction\n",
    "        self.probe = LinearProbe(feature_dim)  # Linear probe to map features to saliency predictions\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor representing an image or batch of images.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A saliency map (or batch of saliency maps) with values in the range [0, 1].\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)  # Extract features using the backbone network\n",
    "        return torch.sigmoid(self.probe(features))  # Apply the linear probe and sigmoid to get saliency map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go throught SALICON dataset and how we processed it.\n",
    "SALICON provides :\n",
    "- 10k train images,\n",
    "- 5k validation images \n",
    "- 5K test images \n",
    "- 2 Annotation files : one for the training dataset and one for the validation dataset. \n",
    "\n",
    "The SaliencyDataset class takes as an input a dataset (train, val or test) and returns images and their corresponding saliency maps. \n",
    "\n",
    "The saliency maps are generated inside the SaliencyDataset class using the function points_to_saliency that we defined. This function takes a list of fixation points (e.g., (row, col) coordinates) from the annotation files and generates a smooth saliency map of the specified size.The saliency map highlights regions where fixations are concentrated, with Gaussian smoothing applied to create a continuous distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None):\n",
    "\n",
    "        self.image_dir = image_dir  # Directory where images are stored\n",
    "        self.transform = transform  # Optional transformations (e.g., resizing, normalization)\n",
    "        \n",
    "        # Load annotations from the JSON file\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.annotations = json.load(f)\n",
    "        \n",
    "        # Organize fixations by image ID\n",
    "        self.fixations = {}\n",
    "        for ann in self.annotations['annotations']:\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.fixations:\n",
    "                self.fixations[image_id] = []\n",
    "            self.fixations[image_id].extend(ann['fixations'])  # Add fixations for the image\n",
    "        \n",
    "        # Map image IDs to file names and original dimensions\n",
    "        self.image_id_to_file = {\n",
    "            img['id']: (img['file_name'], img['height'], img['width']) \n",
    "            for img in self.annotations['images']\n",
    "        }\n",
    "        \n",
    "        # List of all image IDs\n",
    "        self.image_ids = list(self.fixations.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get image ID and corresponding file name and dimensions\n",
    "        image_id = self.image_ids[idx]\n",
    "        image_filename, original_height, original_width = self.image_id_to_file[image_id]\n",
    "        \n",
    "        # Load the image\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure image is in RGB format\n",
    "        \n",
    "        # Get fixation points for the image\n",
    "        points = self.fixations.get(image_id, [])\n",
    "        \n",
    "        # Convert fixation points to a saliency map\n",
    "        saliency_map = points_to_saliency(\n",
    "            points, \n",
    "            image_size=(256, 256), \n",
    "            original_image_height=original_height, \n",
    "            original_image_width=original_width\n",
    "        )\n",
    "        \n",
    "        # Convert saliency map to a PIL image\n",
    "        saliency_map = Image.fromarray((saliency_map * 255).astype(np.uint8))\n",
    "        \n",
    "        # Apply transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            saliency_map = self.transform(saliency_map)\n",
    "        \n",
    "        return image, saliency_map\n",
    "\n",
    "# Convert Fixation Points to Saliency Map\n",
    "def points_to_saliency(points, image_size=(256, 256), original_image_height=None, original_image_width=None, sigma=10):\n",
    "\n",
    "    # Initialize an empty saliency map\n",
    "    saliency_map = np.zeros(image_size, dtype=np.float32)\n",
    "    \n",
    "    # Scale fixation points to the target image size\n",
    "    for (row, col) in points:\n",
    "        row = int((row - 1) * (image_size[0] / original_image_height))\n",
    "        col = int((col - 1) * (image_size[1] / original_image_width))\n",
    "        \n",
    "        # Ensure points are within bounds\n",
    "        row = min(max(row, 0), image_size[0] - 1)\n",
    "        col = min(max(col, 0), image_size[1] - 1)\n",
    "        \n",
    "        # Increment the saliency map at the fixation point\n",
    "        saliency_map[row, col] += 1.0\n",
    "    \n",
    "    # Apply Gaussian smoothing to the saliency map\n",
    "    saliency_map = cv2.GaussianBlur(saliency_map, (0, 0), sigmaX=sigma, sigmaY=sigma)\n",
    "    \n",
    "    # Normalize the saliency map to [0, 1]\n",
    "    if saliency_map.max() > 0:\n",
    "        saliency_map /= saliency_map.max()\n",
    "    \n",
    "    return saliency_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training : Defining the training loop \n",
    "- 10 Epochs\n",
    "- batch size = 32\n",
    "- learning rate = 0.0001\n",
    "- Optimizer : Adaptive Moment Estimation (Adam)\n",
    "- Criterion : Mean Squared Error (MSE) Loss\n",
    "\n",
    "We also applied transformations to the images (see transform variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, saliency_maps in dataloader:\n",
    "        images, saliency_maps = images.to(device), saliency_maps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, saliency_maps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.0001\n",
    "    num_epochs = 10\n",
    "    image_dir = \"../train\"  #Define the path to the train dataset \n",
    "    annotation_file = \".../fixations_train2014.json\" # Define the path to the annotation file\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = SaliencyDataset(image_dir, annotation_file, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = SaliencyPredictor(backbone, feature_dim).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train(model, dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # âœ… Save Trained Model\n",
    "    torch.save(model.state_dict(), \"/resnet_saliency.pth\") # Define a path and save the model \n",
    "    print(\"âœ… Model saved!\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation : Defining the validation loop \n",
    "We're gonna use the 5K validation images in the loop, and we're gonna save the output of the model (the predicted saliency maps) and save the visualizations of the predicted heatmap + ground truth heatmap + original image of every validation image in a folder. \n",
    "\n",
    "We're gonna also calculate some metrics such as MSE loss, pearson correlation and AUC to save them later in a file so we can analyse the performance of the model (see the notebook Performance_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining validation Data Paths and load the data\n",
    "\n",
    "val_image_dir = \"/Users/nouira/Desktop/deeplearning/project/val\"\n",
    "val_annotation_file = \"/Users/nouira/Desktop/deeplearning/project/fixations_val2014.json\"\n",
    "\n",
    "val_dataset = SaliencyDataset(val_image_dir, val_annotation_file, transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model.eval() # put model on evaluation mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Evaluation Metrics\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "mse_total = 0\n",
    "pearson_corrs = [] # a list to save the pearson correlation values of every image\n",
    "auc_scores = [] # a list to save the AUC values of every image\n",
    "\n",
    "# Directory to Save Visualizations\n",
    "output_dir = \".../Predicted_saliency_maps\" # Define the path of the output for to save saliency maps in validation\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to store metrics for each image\n",
    "image_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Validation Loop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Run Model on Validation Set\n",
    "print(\"ðŸ”¹ Running validation...\")\n",
    "with torch.no_grad():\n",
    "    for images, targets, image_ids in val_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        # âœ… Use full model instead of just the probe\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Compute MSE Loss\n",
    "        mse = mse_loss(predictions, targets)\n",
    "        mse_total += mse.item()\n",
    "\n",
    "        # âœ… Save Visualizations and Compute Metrics for Each Image\n",
    "        for i in range(len(image_ids)):\n",
    "            # Get the image ID\n",
    "            image_id = image_ids[i].item()\n",
    "\n",
    "            # Extract the ground truth and predicted saliency maps\n",
    "            gt_map = targets[i].cpu().squeeze().numpy()\n",
    "            pred_map = predictions[i].cpu().squeeze().numpy()\n",
    "\n",
    "            # Compute Pearson Correlation\n",
    "            pred_flat = pred_map.flatten()\n",
    "            target_flat = gt_map.flatten()\n",
    "            if np.any(target_flat > 0):  # Avoid division by zero\n",
    "                pearson_corr, _ = pearsonr(pred_flat, target_flat)\n",
    "            else:\n",
    "                pearson_corr = np.nan  # Avoid division by zero\n",
    "\n",
    "            # Compute AUC Score\n",
    "            auc = roc_auc_score((target_flat > 0.5).astype(int), pred_flat)\n",
    "\n",
    "            # Store metrics in the dictionary\n",
    "            image_metrics[image_id] = {\n",
    "                \"mse\": float(np.mean((pred_flat - target_flat) ** 2)),  # MSE for this image\n",
    "                \"pearson_corr\": float(pearson_corr),  # Pearson Correlation for this image\n",
    "                \"auc\": float(auc),  # AUC for this image\n",
    "            }\n",
    "\n",
    "            # Save Visualization\n",
    "            img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "            axes[0].imshow(img)\n",
    "            axes[0].set_title(\"Input Image\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].imshow(gt_map, cmap=\"jet\")\n",
    "            axes[1].set_title(\"Ground Truth Saliency\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            axes[2].imshow(pred_map, cmap=\"jet\")\n",
    "            axes[2].set_title(\"Predicted Saliency\")\n",
    "            axes[2].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f\"saliency_{image_id}.png\"))\n",
    "            plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average final metrics and save the results in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Compute Final Metrics (averages)\n",
    "avg_mse = mse_total / len(val_loader)\n",
    "avg_pearson = np.mean([metrics[\"pearson_corr\"] for metrics in image_metrics.values()])\n",
    "avg_auc = np.mean([metrics[\"auc\"] for metrics in image_metrics.values()])\n",
    "\n",
    "# âœ… Save Results to File\n",
    "results_path = \"/../validation_results_resnet.json\" # Define a path \n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"average_metrics\": {\n",
    "            \"mse\": avg_mse,\n",
    "            \"pearson_corr\": avg_pearson,\n",
    "            \"auc\": avg_auc,\n",
    "        },\n",
    "        \"image_metrics\": image_metrics,  # Metrics for each image\n",
    "    }, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Validation results saved to {results_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
